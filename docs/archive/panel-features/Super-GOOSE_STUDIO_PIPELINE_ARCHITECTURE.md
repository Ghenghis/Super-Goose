# GOOSE STUDIO â€” Agentic LoRA Pipeline Architecture

## "Every Path Leads to a Working Core"

**Version:** 1.0.0
**Date:** February 11, 2026
**Status:** Architecture Design
**Parent System:** Super-Goose L6 Desktop

---

## 1. Executive Summary

Goose Studio is a **tab-based, agentic pipeline** that takes users from zero to a finished, working LoRA adapter (Intelligence Core) through a series of simple guided steps. The system is designed around one absolute principle:

> **"All choices always result in a finished and working product."**

There are no dead ends. Every branch, every option, every path the user can take terminates in a working, validated, deployable LoRA Core. If something fails, agents fix it automatically. If the user doesn't know what to choose, smart defaults produce excellent results.

### What Makes This Different

| Traditional ML Workflow | Goose Studio |
|------------------------|--------------|
| 47 steps, CLI commands, YAML editing | 6 tabs, click through |
| Breaks constantly, cryptic errors | Agents auto-fix all failures |
| Requires ML expertise | Requires zero ML knowledge |
| Dead ends everywhere | Every path completes |
| Manual everything | Fully agentic |
| Cloud-only ($$$) | Local-first (free on your GPUs) |

---

## 2. Design Philosophy: "No Dead Ends"

### The Guarantee System

Every step in the pipeline implements a **Guarantee Contract**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GUARANTEE CONTRACT                   â”‚
â”‚                                                   â”‚
â”‚  For every user choice at step N:                 â”‚
â”‚                                                   â”‚
â”‚  1. There EXISTS a valid path to step N+1         â”‚
â”‚  2. If the path fails, an AGENT intervenes        â”‚
â”‚  3. If the agent fails, a FALLBACK activates      â”‚
â”‚  4. If the fallback fails, the user is TOLD       â”‚
â”‚     exactly what's needed (never left stuck)      â”‚
â”‚                                                   â”‚
â”‚  Failure is NOT: "Error. Try again."              â”‚
â”‚  Failure IS: "This needs X. Want me to fix it?"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Agent Hierarchy

When something goes wrong at any step, a cascade of agents handles it:

```
User Action
    â”‚
    â–¼
[Step Agent] â”€â”€â”€â”€ Handles the normal happy path
    â”‚ fails?
    â–¼
[Repair Agent] â”€â”€ Diagnoses and fixes the issue automatically
    â”‚ fails?
    â–¼
[Fallback Agent] â”€â”€ Tries alternative approaches
    â”‚ fails?
    â–¼
[Guide Agent] â”€â”€â”€ Explains exactly what's needed in plain English
                   Offers to help the user get unstuck
                   NEVER just shows an error and stops
```

---

## 3. The 6-Tab Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   1    â”‚â”€â”€â–¶â”‚    2    â”‚â”€â”€â–¶â”‚    3    â”‚â”€â”€â–¶â”‚    4    â”‚â”€â”€â–¶â”‚   5    â”‚â”€â”€â–¶â”‚    6    â”‚
â”‚ SOURCE â”‚   â”‚  BUILD  â”‚   â”‚ PREPARE â”‚   â”‚  TRAIN  â”‚   â”‚  TEST  â”‚   â”‚ PUBLISH â”‚
â”‚        â”‚   â”‚         â”‚   â”‚         â”‚   â”‚         â”‚   â”‚        â”‚   â”‚         â”‚
â”‚ Where  â”‚   â”‚ Build & â”‚   â”‚ Create  â”‚   â”‚ Run the â”‚   â”‚ Verify â”‚   â”‚ Package â”‚
â”‚ from?  â”‚   â”‚ Preview â”‚   â”‚ trainingâ”‚   â”‚ LoRA    â”‚   â”‚ it     â”‚   â”‚ & Ship  â”‚
â”‚        â”‚   â”‚ the app â”‚   â”‚ data    â”‚   â”‚ trainingâ”‚   â”‚ works  â”‚   â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚              â”‚             â”‚             â”‚             â”‚
  Choose        Nixpacks       Auto or       Click         Chat +       One-click
  source        auto-build     manual        Start         Eval         publish
```

### Tab Overview

| Tab | Name | User Action | Agent Action | Output |
|-----|------|-------------|--------------|--------|
| 1 | SOURCE | Pick: HuggingFace / GitHub / Local | Fetch, validate, scan | Raw materials ready |
| 2 | BUILD | Watch (or skip) | Nixpacks build, Docker run, preview | Running app + code index |
| 3 | PREPARE | Choose dataset strategy | Generate/clean/format training data | Ready JSONL dataset |
| 4 | TRAIN | Pick model + click Start | LLaMA-Factory + Unsloth training | LoRA adapter files |
| 5 | TEST | Chat with it, review scores | Automated eval + vision QA | Quality report |
| 6 | PUBLISH | Set price, write description | Package .gcpkg, validate, upload | Published Core |

---

## 4. Tab 1: SOURCE â€” "Where Does Your Knowledge Come From?"

### User-Facing UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘  SOURCE                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Where should your Intelligence Core learn from?              â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ğŸ¤— HuggingFace â”‚  â”‚  ğŸ™ GitHub      â”‚  â”‚  ğŸ“ My Files â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â”‚  Browse models  â”‚  â”‚  Clone a repo   â”‚  â”‚  Upload your â”‚ â”‚
â”‚  â”‚  and datasets   â”‚  â”‚  and learn its  â”‚  â”‚  own trainingâ”‚ â”‚
â”‚  â”‚  from the Hub   â”‚  â”‚  source code    â”‚  â”‚  data files  â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â”‚  Best for:      â”‚  â”‚  Best for:      â”‚  â”‚  Best for:   â”‚ â”‚
â”‚  â”‚  General skills â”‚  â”‚  Code expertise â”‚  â”‚  Custom data â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”€â”€ Or try a Quick Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                               â”‚
â”‚  [ğŸ”¥ Coding Assistant]  [âœï¸ Writing Style]  [ğŸ§  Reasoning]   â”‚
â”‚  [ğŸ“Š Data Analyst]      [ğŸ® Game Dev]       [ğŸ”§ DevOps]      â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Source: HuggingFace

**User picks:** Browse/search HuggingFace for a base model AND/OR a training dataset.

**Agent actions:**
1. Search HuggingFace Hub for models matching user's goal
2. Auto-filter to models that FIT the user's GPU (VRAM check)
3. Download model + dataset
4. Validate compatibility
5. If dataset needs reformatting â†’ agent reformats automatically

**Available choices (all lead to completion):**

```
HuggingFace Source
â”œâ”€â”€ Pick a Base Model
â”‚   â”œâ”€â”€ Recommended for you: [auto-selected based on GPU + goal]
â”‚   â”œâ”€â”€ Browse by category: Coding / Chat / Reasoning / Vision
â”‚   â”œâ”€â”€ Search by name: _______________
â”‚   â””â”€â”€ Agent guardrail: Only shows models that FIT your VRAM
â”‚
â”œâ”€â”€ Pick a Dataset
â”‚   â”œâ”€â”€ Recommended: [matched to your chosen model + goal]
â”‚   â”œâ”€â”€ Browse popular: coding-instruct, alpaca, dolly, ...
â”‚   â”œâ”€â”€ Search: _______________
â”‚   â””â”€â”€ Agent guardrail: Auto-reformats any dataset to required format
â”‚
â””â”€â”€ Quick Combo (one click)
    â”œâ”€â”€ "Coding Pro" â†’ Qwen3-8B + code-feedback dataset
    â”œâ”€â”€ "Writer" â†’ LLaMA-3-8B + writing-prompts dataset
    â”œâ”€â”€ "Reasoner" â†’ GLM-4-9B + reasoning-chains dataset
    â””â”€â”€ Agent builds everything from the combo template
```

### Source: GitHub

**User picks:** Paste a GitHub URL (or search for repos).

**Agent actions:**
1. Clone the repository
2. **Nixpacks** auto-detects language/framework
3. Analyze code structure (files, functions, classes, patterns)
4. Generate a code knowledge graph
5. Auto-select best base model for this language/framework
6. Proceed to BUILD tab (Tab 2)

**Available choices (all lead to completion):**

```
GitHub Source
â”œâ”€â”€ Paste URL: https://github.com/user/repo
â”‚   â””â”€â”€ Agent: clone â†’ detect â†’ index â†’ ready
â”‚
â”œâ”€â”€ Search GitHub: _______________
â”‚   â””â”€â”€ Agent: search â†’ show results â†’ user picks â†’ clone
â”‚
â”œâ”€â”€ My Repositories (if GitHub connected)
â”‚   â””â”€â”€ Shows your repos with size/language badges
â”‚
â””â”€â”€ Multi-Repo (advanced)
    â”œâ”€â”€ Add multiple repos to learn from
    â””â”€â”€ Agent merges knowledge from all repos
```

### Source: My Files

**User picks:** Upload files (JSONL, CSV, TXT, PDF, code files).

**Agent actions:**
1. Detect file format
2. Auto-convert to training format (instruction/output JSONL)
3. Validate data quality
4. Show preview of training examples
5. Suggest improvements

**Supported uploads:**

```
My Files Source
â”œâ”€â”€ Training Data (structured)
â”‚   â”œâ”€â”€ .jsonl (instruction/output pairs) â†’ ready immediately
â”‚   â”œâ”€â”€ .csv (columns mapped to instruction/output) â†’ auto-convert
â”‚   â””â”€â”€ .parquet â†’ auto-convert
â”‚
â”œâ”€â”€ Documents (unstructured â†’ agent generates Q&A)
â”‚   â”œâ”€â”€ .txt, .md â†’ extract knowledge, generate training pairs
â”‚   â”œâ”€â”€ .pdf â†’ OCR + extract + generate pairs
â”‚   â””â”€â”€ .docx â†’ extract + generate pairs
â”‚
â”œâ”€â”€ Code Files (agent generates code Q&A)
â”‚   â”œâ”€â”€ .py, .js, .ts, .rs, .cs, etc.
â”‚   â””â”€â”€ .zip of project â†’ extract + analyze + generate pairs
â”‚
â””â”€â”€ Chat Exports (conversation history â†’ training pairs)
    â”œâ”€â”€ Goose chat history export
    â”œâ”€â”€ ChatGPT export
    â””â”€â”€ Any conversation JSON
```

### Quick Recipes (One-Click Start)

Pre-built templates that auto-select source + model + dataset + settings:

| Recipe | Base Model | Dataset Source | Result |
|--------|-----------|---------------|--------|
| ğŸ”¥ Coding Assistant | Qwen3-8B | HuggingFace: code-feedback | Code-specialized Core |
| âœï¸ Writing Style | LLaMA-3.3-8B | HuggingFace: writing-prompts | Writing-specialized Core |
| ğŸ§  Reasoning Pro | GLM-4-9B | HuggingFace: reasoning-chains | Reasoning-specialized Core |
| ğŸ“Š Data Analyst | Qwen3-14B | HuggingFace: sql-instruct | Data/SQL-specialized Core |
| ğŸ® Game Dev | DeepSeek-Coder-8B | GitHub: popular game repos | Game dev-specialized Core |
| ğŸ”§ DevOps Expert | Qwen3-8B | HuggingFace: devops-instruct | DevOps-specialized Core |

Each recipe skips straight to TRAIN tab with everything pre-configured.

---

## 5. Tab 2: BUILD â€” "Nixpacks Build Engine"

This tab is **only active when the source is GitHub**. For HuggingFace/Local sources, this tab shows a brief "Source Ready âœ…" status and auto-advances to PREPARE.

### Purpose

Build and run the cloned GitHub project so agents can:
1. See the running application (vision model)
2. Understand the codebase deeply (code indexer)
3. Generate better training data (from real working code)

### Build Engine: Nixpacks Integration

**Nixpacks** (by Railway, open source, Rust-based) is the build engine:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘¡ BUILD                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Repository: github.com/user/awesome-dashboard                â”‚
â”‚  Detected:   React 18 + TypeScript + Vite + Tailwind CSS     â”‚
â”‚  Status:     â³ Building...                                   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Build Log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€ Live Preview â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ âœ… Cloned repository            â”‚                        â”‚ â”‚
â”‚  â”‚ âœ… Detected: Node.js 20         â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚ âœ… nixpacks plan generated      â”‚   â”‚              â”‚    â”‚ â”‚
â”‚  â”‚ âœ… npm install (347 packages)   â”‚   â”‚  Loading...  â”‚    â”‚ â”‚
â”‚  â”‚ â³ npm run build                â”‚   â”‚              â”‚    â”‚ â”‚
â”‚  â”‚ â³ Starting dev server...       â”‚   â”‚              â”‚    â”‚ â”‚
â”‚  â”‚                                  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚                                  â”‚                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Code Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â” â”‚
â”‚  â”‚ Files: 127 â”‚ Functions: 412 â”‚ Components: 34 â”‚ Tests: 18 â”‚ â”‚
â”‚  â”‚ Framework: React 18  â”‚ State: Redux  â”‚ API: REST + tRPC  â”‚ â”‚
â”‚  â”‚ Entry: src/main.tsx  â”‚ Routes: 12    â”‚ DB: PostgreSQL     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  [ğŸ”„ Rebuild]  [â­ï¸ Skip to Prepare]  [â–¶ï¸ Next: Prepare â†’]    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Nixpacks Auto-Detection

Nixpacks detects and builds 20+ languages automatically:

| Language | Detection File | Build Command | Run Command |
|----------|---------------|---------------|-------------|
| Node.js / React | package.json | npm install && npm build | npm start / npm dev |
| Python / Flask / Django | requirements.txt / pyproject.toml | pip install | python app.py |
| Rust | Cargo.toml | cargo build | cargo run |
| Go | go.mod | go build | ./app |
| Ruby / Rails | Gemfile | bundle install | rails server |
| Java / Spring | pom.xml / build.gradle | mvn package | java -jar |
| C# / .NET | *.csproj / *.sln | dotnet build | dotnet run |
| PHP / Laravel | composer.json | composer install | php artisan serve |
| Elixir / Phoenix | mix.exs | mix deps.get | mix phx.server |
| Static HTML | index.html | (none) | nginx serve |

### Build Failure Recovery (No Dead Ends)

When Nixpacks build fails, the agent cascade activates:

```
Build fails
    â”‚
    â–¼
[Build Repair Agent]
    â”œâ”€â”€ Missing dependency? â†’ Auto-add to nixpacks.toml
    â”œâ”€â”€ Wrong Node version? â†’ Set NIXPACKS_NODE_VERSION
    â”œâ”€â”€ Missing env vars? â†’ Create .env from .env.example
    â”œâ”€â”€ Port conflict? â†’ Remap to available port
    â”œâ”€â”€ Missing system lib? â†’ Add to nixPkgs
    â”‚
    â–¼ still fails?
[Dockerfile Fallback Agent]
    â”œâ”€â”€ Scan repo for existing Dockerfile â†’ use it
    â”œâ”€â”€ Generate Dockerfile from code analysis
    â”œâ”€â”€ Try docker compose if compose file exists
    â”‚
    â–¼ still fails?
[Skip Build Agent]
    â”œâ”€â”€ "This repo can't be run, but I can still analyze the code"
    â”œâ”€â”€ Proceed to PREPARE with code-only analysis
    â”œâ”€â”€ User still gets a working LoRA from the source code
    â””â”€â”€ NO DEAD END â€” just a different path
```

### Vision Agent Exploration (When Build Succeeds)

If the project builds and runs (web app on localhost), the vision agent explores:

```
Vision Agent Exploration Report
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Pages found: 12
â”œâ”€â”€ / (Homepage) â”€â”€â”€â”€ Screenshot captured
â”œâ”€â”€ /login â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Form found: email + password
â”œâ”€â”€ /dashboard â”€â”€â”€â”€â”€â”€ Charts: 3 bar, 2 line, 1 pie
â”œâ”€â”€ /settings â”€â”€â”€â”€â”€â”€â”€ 4 tabs: Profile, Security, Billing, API
â”œâ”€â”€ /users â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Table with pagination (47 users)
â””â”€â”€ /404 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Custom error page

Components mapped: 34
â”œâ”€â”€ <Navbar /> â”€â”€â”€ Present on all pages
â”œâ”€â”€ <Sidebar /> â”€â”€ Collapsible, 8 menu items
â”œâ”€â”€ <DataTable /> â”€ Sortable, filterable, paginated
â””â”€â”€ ...

Issues found: 2
â”œâ”€â”€ /settings: "Dark mode" toggle doesn't persist
â””â”€â”€ /users: Table overflows on mobile viewport

This exploration data will be used to generate
high-quality training pairs about this project.
```

---

## 6. Tab 3: PREPARE â€” "Create Training Data"

### Purpose

Transform raw source material into clean, validated training data ready for LoRA fine-tuning. This is where the "magic" happens â€” agents auto-generate training pairs from whatever the user provided.

### User-Facing UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘¢ PREPARE                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Source: github.com/user/awesome-dashboard (React + TS)       â”‚
â”‚                                                               â”‚
â”‚  How should I create training data?                           â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ğŸ¤– Auto-Generateâ”‚  â”‚  ğŸ“ Manual Edit  â”‚  â”‚  ğŸ”€ Mix    â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚            â”‚ â”‚
â”‚  â”‚  I'll analyze    â”‚  â”‚  Upload or write â”‚  â”‚  Auto +    â”‚ â”‚
â”‚  â”‚  the code and    â”‚  â”‚  your own Q&A    â”‚  â”‚  Manual    â”‚ â”‚
â”‚  â”‚  generate Q&A    â”‚  â”‚  training pairs  â”‚  â”‚  combined  â”‚ â”‚
â”‚  â”‚  pairs for you   â”‚  â”‚                  â”‚  â”‚            â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚  Best for:       â”‚  â”‚  Best for: â”‚ â”‚
â”‚  â”‚  Best for:       â”‚  â”‚  Specific domain â”‚  â”‚  Maximum   â”‚ â”‚
â”‚  â”‚  Code learning   â”‚  â”‚  knowledge       â”‚  â”‚  quality   â”‚ â”‚
â”‚  â”‚  (recommended)   â”‚  â”‚                  â”‚  â”‚            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Auto-Generate Pipeline (For GitHub Sources)

The agent reads the codebase and generates training pairs across 6 categories:

```
Code Analysis â†’ Training Data Generation
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Category 1: "What does this do?" (Function Explanation)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Each function/component in the codebase
Output: Q&A about what it does, how it works
Example:
  Q: "How does the authentication middleware work?"
  A: "The auth middleware in src/middleware/auth.ts
     validates JWT tokens from the Authorization header.
     It first checks the Redis cache for revoked tokens,
     then verifies the signature using jsonwebtoken.verify()
     with the JWT_SECRET env var. On failure, it returns
     401 with a structured error response..."

Category 2: "How do I add X?" (Code Generation)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Patterns found in the codebase
Output: "Add a new X following this project's patterns"
Example:
  Q: "Add a new API endpoint for user notifications"
  A: "Based on this project's patterns:
     1. Create src/routes/notifications.ts
     2. Follow the same router pattern as routes/users.ts
     3. Add NotificationService in src/services/
     4. Create migration in prisma/migrations/..."

Category 3: "Fix this error" (Debugging)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Common errors in this tech stack
Output: Project-specific debugging guidance
Example:
  Q: "TypeError: Cannot read property 'map' of undefined
     in UserList.tsx"
  A: "This project fetches users via useQuery in
     hooks/useUsers.ts. The error means the API returned
     before data was ready. Add a loading check:
     if (isLoading) return <Spinner />;
     Also check that the API endpoint /api/users is
     returning the expected { users: [...] } shape..."

Category 4: "Architecture questions" (Design)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Project structure analysis
Output: Architecture-level Q&A
Example:
  Q: "What database does this project use and why?"
  A: "PostgreSQL via Prisma ORM. The schema is in
     prisma/schema.prisma. Key models: User, Post,
     Comment with cascade deletes. The choice of
     Postgres over MongoDB is because the data is
     highly relational (users have posts which have
     comments). Connection pooling via PgBouncer..."

Category 5: "Test this" (Testing)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Existing tests + untested code
Output: How to write tests for this project
Example:
  Q: "Write a test for the createUser service"
  A: "Using this project's test setup (Jest + Supertest):
     import { createUser } from '../services/userService';
     import { prismaMock } from '../__mocks__/prisma';
     ..."

Category 6: "UI/UX" (From Vision Agent Exploration)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Input:  Screenshots + component analysis from BUILD tab
Output: UI-specific Q&A
Example:
  Q: "How is the dashboard layout structured?"
  A: "The dashboard uses a 3-column grid layout:
     - Left: <Sidebar /> (collapsible, 240px)
     - Center: <MainContent /> (flex-grow)
     - Right: <ActivityFeed /> (320px, hidden on mobile)
     Charts use Recharts library with the theme
     colors defined in styles/theme.ts..."
```

### Auto-Generate Pipeline (For HuggingFace Datasets)

When the source is a HuggingFace dataset, the agent:

1. Downloads the dataset
2. Inspects format (instruction/output, QA, conversation, etc.)
3. Converts to standard JSONL format
4. Filters low-quality entries
5. Balances categories
6. Shows preview for user approval

### Data Quality Dashboard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training Data Ready                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Total pairs: 2,847                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Function Explanation  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 812     â”‚      â”‚
â”‚  â”‚ Code Generation       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   734     â”‚      â”‚
â”‚  â”‚ Debugging             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       523     â”‚      â”‚
â”‚  â”‚ Architecture           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        401     â”‚      â”‚
â”‚  â”‚ Testing                â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           287     â”‚      â”‚
â”‚  â”‚ UI/UX                  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              90      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  Quality Score: 87/100  âœ…                                    â”‚
â”‚  Avg tokens per pair: 342                                     â”‚
â”‚  Estimated training time: ~28 minutes (Qwen3-8B, QLoRA)      â”‚
â”‚                                                               â”‚
â”‚  [ğŸ‘ï¸ Preview Samples]  [âœï¸ Edit/Add]  [â–¶ï¸ Next: Train â†’]    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### No Dead Ends in PREPARE

| Scenario | Agent Response |
|----------|---------------|
| Dataset too small (<100 pairs) | Agent generates more from related HuggingFace datasets |
| Dataset has formatting errors | Agent auto-fixes format |
| Mixed languages in dataset | Agent separates and lets user choose |
| Uploaded CSV with wrong columns | Agent asks which column is which, then maps |
| Empty upload | Agent suggests Quick Recipe datasets |
| Corrupted file | Agent reports issue, offers alternative upload |

---

## 7. Tab 4: TRAIN â€” "Run the LoRA Training"

### User-Facing UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘£ TRAIN                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Recommended: Qwen3-8B (fits 3090 Ti, fast, excellent)  â”‚ â”‚
â”‚  â”‚  [Qwen3-8B â–¼]  â† dropdown with GPU-compatible models    â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  VRAM needed: ~6GB of 24GB available  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 25%  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Method â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â— QLoRA 4-bit (Recommended â€” fast, low VRAM)           â”‚ â”‚
â”‚  â”‚  â—‹ LoRA 16-bit (Higher quality, more VRAM)              â”‚ â”‚
â”‚  â”‚  â—‹ Full fine-tune (Maximum quality, needs most VRAM)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Speed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â— Turbo (Unsloth enabled â€” 2-5x faster)               â”‚ â”‚
â”‚  â”‚  â—‹ Standard                                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Presets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â— Recommended (3 epochs, lr 2e-4, rank 16, alpha 32)  â”‚ â”‚
â”‚  â”‚  â—‹ Quick Test (1 epoch â€” 5 min, lower quality)          â”‚ â”‚
â”‚  â”‚  â—‹ High Quality (5 epochs â€” 45 min, best results)       â”‚ â”‚
â”‚  â”‚  â—‹ Custom (show all hyperparameters)                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  Dataset: 2,847 pairs â”‚ Est. time: ~28 min â”‚ VRAM: 6.2GB    â”‚
â”‚                                                               â”‚
â”‚  [ â–¶â–¶ START TRAINING ]                                       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### During Training

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘£ TRAIN â€” Running                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Model: Qwen3-8B â”‚ Method: QLoRA 4-bit â”‚ Unsloth: ON        â”‚
â”‚  Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 62% (Step 1,764/2,847)â”‚
â”‚  Time: 17:23 elapsed â”‚ ~10:30 remaining                      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Loss Curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 2.4 â•·                                                    â”‚ â”‚
â”‚  â”‚     â”‚â•²                                                   â”‚ â”‚
â”‚  â”‚ 1.8 â”‚ â•²                                                  â”‚ â”‚
â”‚  â”‚     â”‚  â•²                                                 â”‚ â”‚
â”‚  â”‚ 1.2 â”‚   â•²__                                              â”‚ â”‚
â”‚  â”‚     â”‚      â•²___                                          â”‚ â”‚
â”‚  â”‚ 0.6 â”‚          â•²_______                                  â”‚ â”‚
â”‚  â”‚     â”‚                  â•²___________                      â”‚ â”‚
â”‚  â”‚ 0.0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ steps           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ GPU Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  RTX 3090 Ti: 6.2GB / 24GB  [â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 26%  72Â°C    â”‚ â”‚
â”‚  â”‚  RTX 3060 Ti: idle (available for inference)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  [â¸ Pause]  [â¹ Stop & Save Best]  [ğŸ“Š Details]              â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Selection: GPU-Aware Filtering

The dropdown ONLY shows models that fit the user's hardware:

```
Model Selector (for RTX 3090 Ti, 24GB)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â­ Recommended
â”œâ”€â”€ Qwen3-8B          â”‚ ~6GB  â”‚ âš¡ Fast  â”‚ Best all-around
â”œâ”€â”€ GLM-4-9B-0414     â”‚ ~7GB  â”‚ âš¡ Fast  â”‚ Great for coding
â””â”€â”€ LLaMA-3.3-8B      â”‚ ~6GB  â”‚ âš¡ Fast  â”‚ Great for English

More Models
â”œâ”€â”€ Qwen3-14B         â”‚ ~10GB â”‚ ğŸŸ¡ Med   â”‚ Higher quality
â”œâ”€â”€ DeepSeek-V3-7B    â”‚ ~6GB  â”‚ âš¡ Fast  â”‚ Strong reasoning
â”œâ”€â”€ Phi-4-14B         â”‚ ~10GB â”‚ ğŸŸ¡ Med   â”‚ Microsoft, compact
â”œâ”€â”€ Mistral-7B-v0.3   â”‚ ~6GB  â”‚ âš¡ Fast  â”‚ European model
â””â”€â”€ Qwen3-30B-A3B     â”‚ ~18GB â”‚ ğŸ”´ Slow  â”‚ MoE, big brain

Advanced (tight fit)
â”œâ”€â”€ GLM-4-32B-0414    â”‚ ~20GB â”‚ ğŸ”´ Slow  â”‚ Max quality 
â””â”€â”€ LLaMA-3.1-70B     â”‚ ~22GB â”‚ ğŸ”´ Slow  â”‚ Needs Unsloth

â”€â”€â”€â”€â”€ Hidden (won't fit) â”€â”€â”€â”€â”€
  âœ— Qwen3-72B (needs 40GB+)
  âœ— LLaMA-3.1-405B (needs cluster)
  âœ— DeepSeek-V3-671B (needs cluster)
```

### Backend: LLaMA-Factory + Unsloth

Every training run generates a YAML recipe:

```yaml
# Auto-generated by Goose Studio
# Recipe: coding-assistant-qwen3-8b-qlora
model_name_or_path: Qwen/Qwen3-8B
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05

# QLoRA settings
quantization_bit: 4
quantization_method: bitsandbytes

# Unsloth acceleration
use_unsloth: true

# Dataset
dataset: goose_studio_prepared
template: qwen
cutoff_len: 2048

# Training
num_train_epochs: 3
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true

# Output
output_dir: /studio/output/coding-assistant-v1
logging_steps: 10
save_steps: 500

# Monitoring
report_to: tensorboard
```

### Training Failure Recovery (No Dead Ends)

| Failure | Agent Response |
|---------|---------------|
| OOM (out of VRAM) | Auto-reduce batch size â†’ retry. If still OOM, switch to smaller model |
| CUDA error | Reset GPU, clear cache, retry. If persistent, switch to CPU offload |
| Loss explodes (NaN) | Reduce learning rate by half â†’ retry from last checkpoint |
| Loss plateaus (not learning) | Increase learning rate or add more data â†’ retry |
| Disk full | Move cache, clear old outputs, retry |
| Training hangs | Kill process, restart from last checkpoint |
| Model download fails | Retry with mirror, or suggest pre-downloading |
| Unsloth incompatible | Disable Unsloth, train standard (slower but works) |

**Key principle:** The agent ALWAYS recovers or finds an alternative. The user never sees a bare error message.

---

## 8. Tab 5: TEST â€” "Verify It Works"

### User-Facing UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘¤ TEST                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Training Complete âœ…  Adapter: coding-assistant-v1 (8.3MB)  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Chat Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€ Auto Eval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                            â”‚                             â”‚ â”‚
â”‚  â”‚  You: How do I add a new  â”‚  Benchmark Results          â”‚ â”‚
â”‚  â”‚  API route to this        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚ â”‚
â”‚  â”‚  project?                 â”‚                             â”‚ â”‚
â”‚  â”‚                            â”‚  Code Quality:  92/100 âœ…  â”‚ â”‚
â”‚  â”‚  Core: Based on this      â”‚  Instruction     88/100 âœ…  â”‚ â”‚
â”‚  â”‚  project's patterns in    â”‚   Following:                â”‚ â”‚
â”‚  â”‚  src/routes/, you'd       â”‚  Helpfulness:   85/100 âœ…  â”‚ â”‚
â”‚  â”‚  create a new file...     â”‚  Accuracy:      90/100 âœ…  â”‚ â”‚
â”‚  â”‚  [shows detailed,         â”‚  Consistency:   87/100 âœ…  â”‚ â”‚
â”‚  â”‚   project-aware answer]   â”‚                             â”‚ â”‚
â”‚  â”‚                            â”‚  Overall:       88/100 âœ…  â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                             â”‚ â”‚
â”‚  â”‚  â”‚ Type a question...   â”‚ â”‚  vs Base Model: +23 points â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  Grade: â­â­â­â­ Excellent  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Side-by-Side Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Base Model (no LoRA)    â”‚  Your Core (with LoRA)       â”‚ â”‚
â”‚  â”‚  "To add an API route,   â”‚  "In this project, routes    â”‚ â”‚
â”‚  â”‚  you typically create    â”‚  live in src/routes/. Create  â”‚ â”‚
â”‚  â”‚  an endpoint handler..." â”‚  notifications.ts following   â”‚ â”‚
â”‚  â”‚  (generic answer)        â”‚  the pattern in users.ts..."  â”‚ â”‚
â”‚  â”‚                           â”‚  (project-specific answer!)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  [ğŸ”„ Retrain (adjust)]  [â–¶ï¸ Next: Publish â†’]                 â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Automated Eval Pipeline

The agent runs multiple evaluation methods:

```
Evaluation Pipeline
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. HELD-OUT TEST SET (10% of training data reserved)
   â†’ Run model on unseen examples
   â†’ Compare outputs to expected answers
   â†’ Calculate BLEU, ROUGE, semantic similarity
   
2. BENCHMARK EVAL (standard tests)
   â†’ HumanEval (coding)
   â†’ MMLU subset (general knowledge)
   â†’ GSM8K subset (reasoning)
   â†’ IFEval (instruction following)
   â†’ Compare LoRA vs base model

3. CHAT QUALITY (LLM-as-judge)
   â†’ Generate 20 responses to diverse prompts
   â†’ Another LLM rates helpfulness, accuracy, style
   â†’ Produces quality score 0-100

4. VISION QA (if web project, uses GPU 2)
   â†’ Vision model reviews the running app
   â†’ Asks the Core questions about the UI
   â†’ Verifies Core gives correct, specific answers

5. REGRESSION CHECK
   â†’ Ensure LoRA didn't break base model capabilities
   â†’ Test general knowledge still works
   â†’ Test multi-language still works
```

### If Quality is Low

The agent doesn't just report failure â€” it fixes it:

```
Quality Score: 54/100 âš ï¸ Below threshold (70)

Agent Analysis:
  "The Core is not learning the project-specific patterns
   well enough. This is likely because:
   
   1. Dataset has too many generic examples (42% generic)
   2. Training was only 1 epoch (underfitting)
   
   I recommend:
   âœ… Filtering dataset to remove generic pairs
   âœ… Increasing to 3 epochs
   âœ… Increasing LoRA rank from 8 to 16
   
   [ğŸ”§ Auto-Fix & Retrain] â† one click, agent handles it
   [âœï¸ Manual Adjustments]
   [â­ï¸ Accept Anyway]"
```

---

## 9. Tab 6: PUBLISH â€” "Package & Ship"

### User-Facing UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘¥ PUBLISH                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  Quality Score: 88/100 âœ… Ready to publish!                  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Core Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Name:  [React Dashboard Expert_________]                â”‚ â”‚
â”‚  â”‚  Desc:  [Specialized in React+TS dashboard patterns.    â”‚ â”‚
â”‚  â”‚          Trained on awesome-dashboard repo. Knows        â”‚ â”‚
â”‚  â”‚          Recharts, Redux, Prisma, tRPC patterns._____]  â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Tags:  [react] [typescript] [dashboard] [coding]        â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  Icon:  [ğŸ¯] â† pick emoji or upload                     â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Validation Gates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  âœ… V1: Schema valid (manifest.json, adapter files)     â”‚  â”‚
â”‚  â”‚  âœ… V2: Security passed (no secrets, no malicious code) â”‚  â”‚
â”‚  â”‚  âœ… V3: Eval passed (88/100, above 70 threshold)        â”‚  â”‚
â”‚  â”‚  âœ… V4: Portable (works with base model on any GPU)     â”‚  â”‚
â”‚  â”‚  âœ… V5: Quality (consistent outputs, no degradation)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Pricing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â— Free (open source, anyone can use)                    â”‚ â”‚
â”‚  â”‚  â—‹ $4.99 (suggested by pricing engine)                   â”‚ â”‚
â”‚  â”‚  â—‹ Custom: $[___]                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€ Destination â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â˜‘ Goose Marketplace                                     â”‚ â”‚
â”‚  â”‚  â˜‘ Save locally (G:\goose\studio\cores\)                 â”‚ â”‚
â”‚  â”‚  â˜ Upload to HuggingFace Hub                             â”‚ â”‚
â”‚  â”‚  â˜ Export as standalone GGUF                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  [ ğŸš€ PUBLISH CORE ]                                         â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Package (.gcpkg) â€” Final Output

```
react-dashboard-expert-v1.gcpkg (12.4 MB)
â”œâ”€â”€ manifest.json              # Core identity + metadata
â”‚   {
â”‚     "name": "React Dashboard Expert",
â”‚     "version": "1.0.0",
â”‚     "base_model": "Qwen/Qwen3-8B",
â”‚     "method": "qlora-4bit",
â”‚     "quality_score": 88,
â”‚     "tags": ["react", "typescript", "dashboard"],
â”‚     "created": "2026-02-11T23:45:00Z",
â”‚     "hardware": { "trained_on": "RTX 3090 Ti" },
â”‚     "compatible_vram_min_gb": 6
â”‚   }
â”‚
â”œâ”€â”€ adapter/
â”‚   â”œâ”€â”€ adapter_model.safetensors   # LoRA weights (8.3MB)
â”‚   â””â”€â”€ adapter_config.json         # PEFT configuration
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ recipe.yaml                 # LLaMA-Factory config (reproducible)
â”‚   â”œâ”€â”€ training_log.jsonl          # Full training log
â”‚   â””â”€â”€ eval_results.json           # Benchmark scores
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset_card.md             # What data was used (not the data)
â”‚   â””â”€â”€ sample_pairs.jsonl          # 10 example pairs for preview
â”‚
â”œâ”€â”€ README.md                       # Core description + usage
â””â”€â”€ PROVENANCE.json                 # Full audit trail
    {
      "source": "github.com/user/awesome-dashboard",
      "commit": "a1b2c3d",
      "build_engine": "nixpacks",
      "training_duration_sec": 1680,
      "training_gpu": "NVIDIA RTX 3090 Ti",
      "dataset_pairs": 2847,
      "epochs": 3,
      "final_loss": 0.42,
      "unsloth": true
    }
```

---

## 10. Infrastructure: Docker Service Architecture

### docker-compose.studio.yml

```yaml
version: "3.8"

services:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # CORE: LLaMA-Factory Training Engine
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  llamafactory:
    image: hiyouga/llamafactory:latest
    container_name: goose-studio-train
    ports:
      - "7860:7860"   # LlamaBoard WebUI
      - "8000:8000"   # OpenAI-compatible API
    volumes:
      - ${GOOSE_HOME}/studio/data:/app/data
      - ${GOOSE_HOME}/studio/output:/app/output
      - ${GOOSE_HOME}/studio/recipes:/app/recipes
      - ${GOOSE_HOME}/studio/cache:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - GRADIO_SERVER_NAME=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]    # RTX 3090 Ti
              capabilities: [gpu]
    restart: unless-stopped

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # BUILD ENGINE: Nixpacks + Docker-in-Docker
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  builder:
    image: ghcr.io/railwayapp/nixpacks:latest
    container_name: goose-studio-builder
    volumes:
      - ${GOOSE_HOME}/studio/repos:/repos
      - ${GOOSE_HOME}/studio/builds:/builds
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
    restart: unless-stopped

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # SANDBOX: Running user projects
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  sandbox:
    build:
      context: ./docker/sandbox
    container_name: goose-studio-sandbox
    ports:
      - "3000-3010:3000-3010"   # App ports
      - "6080:6080"              # noVNC (desktop apps)
    volumes:
      - ${GOOSE_HOME}/studio/repos:/workspace
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # VISION: UI Navigation Agent
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  vision-agent:
    image: goose/vision-agent:latest
    container_name: goose-studio-vision
    ports:
      - "7870:7870"   # Vision agent API
    volumes:
      - ${GOOSE_HOME}/studio/screenshots:/screenshots
    environment:
      - CUDA_VISIBLE_DEVICES=1
      - MODEL=Qwen/Qwen2.5-VL-7B-Instruct
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["1"]    # RTX 3060 Ti
              capabilities: [gpu]
    restart: unless-stopped

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # DATA PREP: Training data generation
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  data-agent:
    image: goose/data-agent:latest
    container_name: goose-studio-data
    ports:
      - "7871:7871"
    volumes:
      - ${GOOSE_HOME}/studio/data:/data
      - ${GOOSE_HOME}/studio/repos:/repos:ro
    restart: unless-stopped

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # VALIDATOR: 5-Gate Core Validation
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  validator:
    image: goose/core-validator:latest
    container_name: goose-studio-validator
    ports:
      - "7872:7872"
    volumes:
      - ${GOOSE_HOME}/studio/output:/output:ro
      - ${GOOSE_HOME}/studio/cores:/cores
    restart: unless-stopped
```

### GPU Assignment

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPU 0: RTX 3090 Ti (24GB)                           â”‚
â”‚  â”œâ”€â”€ Primary: LoRA Training (LLaMA-Factory)          â”‚
â”‚  â”œâ”€â”€ Secondary: Large model inference                â”‚
â”‚  â””â”€â”€ Idle: Available for sandbox GPU tasks           â”‚
â”‚                                                       â”‚
â”‚  GPU 1: RTX 3060 Ti (12GB)                           â”‚
â”‚  â”œâ”€â”€ Primary: Vision Agent (Qwen2.5-VL-7B)          â”‚
â”‚  â”œâ”€â”€ Secondary: Chat testing inference               â”‚
â”‚  â””â”€â”€ Idle: Available for second training job         â”‚
â”‚                                                       â”‚
â”‚  GPU 2+: Tesla P40 (24GB each, optional)             â”‚
â”‚  â”œâ”€â”€ Overflow training (multi-GPU)                   â”‚
â”‚  â”œâ”€â”€ Parallel inference                              â”‚
â”‚  â””â”€â”€ Batch evaluation                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Agent Orchestration System

### Agent Roles

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT HIERARCHY                            â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  CONDUCTOR AGENT (orchestrates the entire pipeline)      â”‚ â”‚
â”‚  â”‚  - Monitors all tabs / all steps                         â”‚ â”‚
â”‚  â”‚  - Decides when to advance to next tab                   â”‚ â”‚
â”‚  â”‚  - Handles cross-step dependencies                       â”‚ â”‚
â”‚  â”‚  - Ensures "no dead ends" guarantee                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚       â”‚       â”‚       â”‚       â”‚       â”‚       â”‚          â”‚
â”‚  â–¼       â–¼       â–¼       â–¼       â–¼       â–¼       â–¼          â”‚
â”‚ Source  Build  Prepare  Train   Test  Publish  Repair       â”‚
â”‚ Agent   Agent  Agent    Agent   Agent  Agent   Agent        â”‚
â”‚                                                               â”‚
â”‚ Fetches Nixpacks Generates Runs    Evals  Packages Fixes    â”‚
â”‚ models  builds   training  LLaMA-  quality validates any    â”‚
â”‚ datasets runs    data      Factory scores  .gcpkg   failure â”‚
â”‚ repos   sandbox            +Unsloth                          â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  VISION AGENT (sees and navigates running applications)  â”‚ â”‚
â”‚  â”‚  - Takes screenshots of preview                          â”‚ â”‚
â”‚  â”‚  - Clicks through UI, maps pages                         â”‚ â”‚
â”‚  â”‚  - Reports bugs and layout issues                        â”‚ â”‚
â”‚  â”‚  - Generates UI-specific training data                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Communication Protocol

Agents communicate through a simple event bus:

```
Event: step.source.complete
  payload: { source_type: "github", repo: "user/repo", files: 127 }
  â†’ triggers: Build Agent starts

Event: step.build.complete
  payload: { success: true, port: 3000, framework: "react" }
  â†’ triggers: Vision Agent explores, Prepare Agent starts

Event: step.build.failed
  payload: { error: "npm install failed", exit_code: 1 }
  â†’ triggers: Repair Agent activates

Event: step.prepare.complete
  payload: { pairs: 2847, quality: 87 }
  â†’ triggers: Train tab becomes active

Event: step.train.progress
  payload: { step: 1764, total: 2847, loss: 0.42, eta_sec: 630 }
  â†’ triggers: UI update (progress bar, loss curve)

Event: step.train.complete
  payload: { adapter_path: "/output/v1/", loss: 0.38 }
  â†’ triggers: Test Agent starts evaluation

Event: step.test.complete
  payload: { score: 88, passed: true }
  â†’ triggers: Publish tab becomes active

Event: step.test.failed
  payload: { score: 54, issues: ["underfitting", "generic data"] }
  â†’ triggers: Repair Agent with auto-fix suggestions
```

---

## 12. Complete User Journey: Start to Finish

### Scenario: "I want a Core that knows React Dashboard development"

```
TIME    ACTION                                              TAB
â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€
0:00    User opens Studio                                   -
0:01    Clicks "GitHub" source                              â‘  SOURCE
0:02    Pastes: github.com/user/awesome-dashboard           â‘  SOURCE
0:05    Agent clones repo (2 sec), detects React+TS         â‘  SOURCE
        â†’ Auto-advances to BUILD

0:06    Nixpacks detects Node.js 20, starts build           â‘¡ BUILD
0:15    npm install completes (347 packages)                â‘¡ BUILD
0:20    npm run dev â†’ localhost:3000 live                   â‘¡ BUILD
0:21    Preview shows the running dashboard                 â‘¡ BUILD
0:22    Vision agent starts exploring (background)          â‘¡ BUILD
0:30    Code analysis complete: 127 files, 412 functions    â‘¡ BUILD
0:35    Vision agent mapped 12 pages, 34 components         â‘¡ BUILD
        â†’ User clicks "Next"

0:36    Agent shows: "Auto-Generate recommended"            â‘¢ PREPARE
0:37    User clicks "Auto-Generate"                         â‘¢ PREPARE
1:00    Agent generates 2,847 training pairs                â‘¢ PREPARE
1:01    Quality dashboard shows: 87/100                     â‘¢ PREPARE
        â†’ User clicks "Next"

1:02    Train tab pre-filled: Qwen3-8B, QLoRA, Turbo       â‘£ TRAIN
1:03    User clicks "START TRAINING"                        â‘£ TRAIN
1:05    Model downloading (if not cached)...                â‘£ TRAIN
1:10    Training begins, loss curve updating                â‘£ TRAIN
29:00   Training complete! Loss: 0.38                       â‘£ TRAIN
        â†’ Auto-advances to TEST

29:01   Chat test window opens, auto-eval starts            â‘¤ TEST
29:05   User chats: "How do I add a notification system?"   â‘¤ TEST
29:06   Core responds with project-specific answer          â‘¤ TEST
30:00   Auto-eval complete: 88/100                          â‘¤ TEST
        â†’ User clicks "Next"

30:01   Publish form pre-filled from repo metadata          â‘¥ PUBLISH
30:02   5-gate validation: all pass âœ…                      â‘¥ PUBLISH
30:03   User sets price: Free                               â‘¥ PUBLISH
30:04   User clicks "PUBLISH CORE"                          â‘¥ PUBLISH
30:10   Core live on Marketplace! ğŸ‰                        â‘¥ PUBLISH

TOTAL TIME: ~30 minutes from paste URL to published Core
CLICKS: ~12 clicks total
ML KNOWLEDGE REQUIRED: Zero
```

---

## 13. Quick Recipe: One-Click Flows

For users who don't want to go through all tabs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Quick Recipes â€” One Click to Start                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ğŸ”¥ Coding Assistant                                      â”‚â”‚
â”‚  â”‚ Qwen3-8B + HuggingFace code-feedback dataset             â”‚â”‚
â”‚  â”‚ QLoRA 4-bit + Unsloth â”‚ ~15 min â”‚ 6GB VRAM              â”‚â”‚
â”‚  â”‚ [Start â†’]                                                 â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ âœï¸  Writing Style Clone                                   â”‚â”‚
â”‚  â”‚ LLaMA-3.3-8B + your writing samples (upload)            â”‚â”‚
â”‚  â”‚ QLoRA 4-bit + Unsloth â”‚ ~20 min â”‚ 6GB VRAM              â”‚â”‚
â”‚  â”‚ [Start â†’]                                                 â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ ğŸ§  Reasoning Boost                                       â”‚â”‚
â”‚  â”‚ GLM-4-9B + reasoning-chains dataset                      â”‚â”‚
â”‚  â”‚ QLoRA 4-bit + Unsloth â”‚ ~25 min â”‚ 7GB VRAM              â”‚â”‚
â”‚  â”‚ [Start â†’]                                                 â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ ğŸ™ Learn a GitHub Repo                                   â”‚â”‚
â”‚  â”‚ Paste URL â†’ auto-build â†’ auto-train â†’ done              â”‚â”‚
â”‚  â”‚ Qwen3-8B + auto-generated data â”‚ ~30 min â”‚ 6GB VRAM     â”‚â”‚
â”‚  â”‚ [Paste URL: _______________] [Start â†’]                    â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ ğŸ“Š Data & SQL Expert                                     â”‚â”‚
â”‚  â”‚ Qwen3-14B + sql-create-context dataset                   â”‚â”‚
â”‚  â”‚ QLoRA 4-bit + Unsloth â”‚ ~35 min â”‚ 10GB VRAM             â”‚â”‚
â”‚  â”‚ [Start â†’]                                                 â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ ğŸ® Game Dev Specialist                                    â”‚â”‚
â”‚  â”‚ DeepSeek-Coder-8B + game programming datasets            â”‚â”‚
â”‚  â”‚ QLoRA 4-bit + Unsloth â”‚ ~20 min â”‚ 6GB VRAM              â”‚â”‚
â”‚  â”‚ [Start â†’]                                                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                               â”‚
â”‚  Quick recipes auto-configure ALL tabs. You can still        â”‚
â”‚  review and adjust at any step before training starts.        â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 14. Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Build Engine** | Nixpacks (by Railway) | Auto-detect, auto-build any repo |
| **Training Engine** | LLaMA-Factory + Unsloth | LoRA/QLoRA fine-tuning |
| **Training UI** | LlamaBoard (embedded) | Training configuration WebUI |
| **Vision Agent** | Qwen2.5-VL-7B / ShowUI-2B | See and navigate running apps |
| **Code Indexer** | tree-sitter + custom | Parse and map codebases |
| **Data Generator** | Custom Python pipeline | Auto-generate training pairs from code |
| **Sandbox** | Docker + noVNC | Isolated build/run environment |
| **Model Registry** | HuggingFace Hub API | Download models and datasets |
| **Eval Engine** | lm-eval-harness + custom | Benchmark and quality scoring |
| **Packaging** | Custom .gcpkg builder | Core packaging and validation |
| **Orchestration** | Python + Redis event bus | Agent communication |
| **Frontend** | Electron WebView (in Goose) | Studio tab UI |
| **GPU Management** | NVIDIA Container Toolkit | GPU assignment per service |

### Open Source Dependencies

Every component is open source:

| Component | License | Repository |
|-----------|---------|-----------|
| Nixpacks | MIT | github.com/railwayapp/nixpacks |
| LLaMA-Factory | Apache 2.0 | github.com/hiyouga/LLaMA-Factory |
| Unsloth | Apache 2.0 | github.com/unslothai/unsloth |
| tree-sitter | MIT | github.com/tree-sitter/tree-sitter |
| ShowUI | MIT | github.com/showlab/ShowUI |
| Qwen2.5-VL | Apache 2.0 | huggingface.co/Qwen/Qwen2.5-VL-7B |
| lm-eval-harness | MIT | github.com/EleutherAI/lm-evaluation-harness |
| noVNC | MPL 2.0 | github.com/novnc/noVNC |
| Docker | Apache 2.0 | github.com/moby/moby |

---

## 15. File System Layout

```
G:\goose\
â”œâ”€â”€ studio/                              # â† NEW: Studio workspace
â”‚   â”œâ”€â”€ repos/                           # Cloned GitHub repositories
â”‚   â”‚   â””â”€â”€ {repo-name}/
â”‚   â”‚       â”œâ”€â”€ .git/
â”‚   â”‚       â”œâ”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ nixpacks.toml            # Auto-generated if needed
â”‚   â”‚       â””â”€â”€ .studio-index.json       # Code analysis cache
â”‚   â”‚
â”‚   â”œâ”€â”€ builds/                          # Nixpacks build outputs
â”‚   â”‚   â””â”€â”€ {repo-name}/
â”‚   â”‚       â”œâ”€â”€ Dockerfile               # Generated by Nixpacks
â”‚   â”‚       â””â”€â”€ .nixpacks-plan.json      # Build plan
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                            # Training datasets
â”‚   â”‚   â”œâ”€â”€ prepared/                    # Ready-to-train JSONL
â”‚   â”‚   â”‚   â””â”€â”€ {dataset-name}.jsonl
â”‚   â”‚   â”œâ”€â”€ raw/                         # User uploads before processing
â”‚   â”‚   â”œâ”€â”€ generated/                   # Agent-generated from code
â”‚   â”‚   â””â”€â”€ curated/                     # HuggingFace downloaded
â”‚   â”‚
â”‚   â”œâ”€â”€ output/                          # Training outputs
â”‚   â”‚   â””â”€â”€ {experiment-name}/
â”‚   â”‚       â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚       â”œâ”€â”€ adapter_config.json
â”‚   â”‚       â”œâ”€â”€ training_args.yaml
â”‚   â”‚       â”œâ”€â”€ trainer_log.jsonl
â”‚   â”‚       â””â”€â”€ eval_results.json
â”‚   â”‚
â”‚   â”œâ”€â”€ recipes/                         # Reusable YAML configs
â”‚   â”‚   â”œâ”€â”€ quick-coding-8b.yaml
â”‚   â”‚   â”œâ”€â”€ quick-writing-8b.yaml
â”‚   â”‚   â”œâ”€â”€ quick-reasoning-9b.yaml
â”‚   â”‚   â””â”€â”€ custom/
â”‚   â”‚
â”‚   â”œâ”€â”€ cores/                           # Packaged .gcpkg files
â”‚   â”‚   â””â”€â”€ react-dashboard-expert-v1.gcpkg
â”‚   â”‚
â”‚   â”œâ”€â”€ screenshots/                     # Vision agent captures
â”‚   â”‚   â””â”€â”€ {repo-name}/
â”‚   â”‚       â”œâ”€â”€ page-home.png
â”‚   â”‚       â”œâ”€â”€ page-login.png
â”‚   â”‚       â””â”€â”€ exploration-report.json
â”‚   â”‚
â”‚   â”œâ”€â”€ cache/                           # HuggingFace model cache
â”‚   â”‚   â””â”€â”€ huggingface/
â”‚   â”‚
â”‚   â”œâ”€â”€ docker-compose.studio.yml        # Studio services
â”‚   â””â”€â”€ studio-config.json               # Studio settings
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ GOOSE_STUDIO_PIPELINE_ARCHITECTURE.md  # This document
```

---

## 16. Implementation Roadmap

### Phase 1: Core Pipeline (Weeks 1-3)

**Goal:** Tab 1 (SOURCE) + Tab 4 (TRAIN) + Tab 6 (PUBLISH) working end-to-end.

- [ ] Studio tab in Goose sidebar with 6 sub-tabs
- [ ] HuggingFace model/dataset browser + downloader
- [ ] LLaMA-Factory Docker service with GPU passthrough
- [ ] Training configuration UI (model selector, presets)
- [ ] Training progress display (loss curve, ETA, GPU stats)
- [ ] .gcpkg packaging from trained adapters
- [ ] Quick Recipes: Coding Assistant, Writing Style
- [ ] Basic error recovery (OOM â†’ reduce batch size)

### Phase 2: Build Engine (Weeks 4-5)

**Goal:** Tab 2 (BUILD) with Nixpacks + live preview.

- [ ] GitHub repo clone + Nixpacks auto-detection
- [ ] Docker sandbox for running built projects
- [ ] Live preview WebView in Studio
- [ ] Code indexer (tree-sitter based)
- [ ] Port forwarding for preview
- [ ] noVNC fallback for desktop applications
- [ ] Build failure recovery agents

### Phase 3: Data Generation (Weeks 6-7)

**Goal:** Tab 3 (PREPARE) with auto-generation pipeline.

- [ ] Auto-generate training pairs from code analysis
- [ ] 6-category generation (explain, generate, debug, arch, test, UI)
- [ ] Dataset quality scoring
- [ ] Manual editing / upload support
- [ ] Data preview and filtering UI
- [ ] HuggingFace dataset download + reformatting

### Phase 4: Testing & Vision (Weeks 8-10)

**Goal:** Tab 5 (TEST) with full evaluation + vision agent.

- [ ] Chat test interface (side-by-side comparison)
- [ ] Automated benchmark evaluation
- [ ] Vision agent (ShowUI / Qwen2.5-VL) integration
- [ ] UI exploration and screenshot mapping
- [ ] Quality-based auto-fix suggestions
- [ ] Regression testing

### Phase 5: Polish & Agent Orchestration (Weeks 11-12)

**Goal:** Full "no dead ends" guarantee across all paths.

- [ ] Conductor agent (cross-tab orchestration)
- [ ] Repair agent (automatic failure recovery)
- [ ] Fallback agent (alternative paths)
- [ ] Guide agent (plain-English explanations)
- [ ] End-to-end integration testing
- [ ] Performance optimization
- [ ] Documentation and video tutorials

---

## 17. Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| Time from URL paste to published Core | < 45 minutes | End-to-end timer |
| User clicks to complete pipeline | < 15 clicks | UI analytics |
| Dead end rate (user gets stuck) | 0% | Error tracking |
| Build success rate (Nixpacks) | > 85% | Build logs |
| Training success rate | > 95% | Training logs |
| Quality score of produced Cores | > 70/100 avg | Eval pipeline |
| User needs to read docs | Never | User testing |
| User needs ML knowledge | Never | User testing |

---

## Appendix A: Supported Model Matrix (RTX 3090 Ti)

| Model | Parameters | QLoRA VRAM | Train Time (2K pairs) | Recommended For |
|-------|-----------|------------|----------------------|-----------------|
| Qwen3-8B | 8B | ~6GB | ~15 min | General, coding |
| GLM-4-9B-0414 | 9B | ~7GB | ~18 min | Coding, Chinese |
| LLaMA-3.3-8B-Instruct | 8B | ~6GB | ~15 min | English, writing |
| DeepSeek-Coder-V2-Lite | 7B | ~6GB | ~14 min | Coding specialist |
| Mistral-7B-v0.3 | 7B | ~6GB | ~14 min | European languages |
| Phi-4-14B | 14B | ~10GB | ~30 min | Compact quality |
| Qwen3-14B | 14B | ~10GB | ~30 min | Higher quality |
| Qwen3-30B-A3B | 30B (3B active) | ~18GB | ~40 min | MoE efficiency |
| GLM-4-32B-0414 | 32B | ~20GB | ~2 hr | Maximum quality |

## Appendix B: Nixpacks Language Detection

| Language | Detection Files | Auto-Install | Auto-Start |
|----------|----------------|-------------|------------|
| Node.js | package.json | npm install | npm start |
| Python | requirements.txt, Pipfile, pyproject.toml | pip install | python/gunicorn |
| Go | go.mod | go build | ./binary |
| Rust | Cargo.toml | cargo build | ./target/release/binary |
| Ruby | Gemfile | bundle install | rails s / ruby app.rb |
| Java | pom.xml, build.gradle | mvn package / gradle build | java -jar |
| PHP | composer.json | composer install | php artisan serve |
| C# | *.csproj | dotnet restore | dotnet run |
| Elixir | mix.exs | mix deps.get | mix phx.server |
| Crystal | shard.yml | shards install | crystal run |
| Dart | pubspec.yaml | dart pub get | dart run |
| Deno | deno.json | (auto) | deno run |
| F# | *.fsproj | dotnet restore | dotnet run |
| Haskell | stack.yaml | stack build | stack exec |
| Scala | build.sbt | sbt compile | sbt run |
| Swift | Package.swift | swift build | swift run |
| Zig | build.zig | zig build | ./zig-out/bin/* |
| Clojure | project.clj | lein deps | lein run |
| Cobol | *.cbl | (custom) | (custom) |
| Static HTML | index.html | (none) | nginx/caddy |

---

*End of Architecture Document*
*"Every Path Leads to a Working Core"*
